# -*- coding: utf-8 -*-
"""CSCI 485.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1okVaDi4PrZMEZ34DGJJaribjz6vJLrTC

Task-1 Dataset Exploration
"""

#1
from sklearn.datasets import load_diabetes
import pandas as pd

diabetes = load_diabetes()
df = pd.DataFrame(diabetes.data, columns=diabetes.feature_names)
df['target'] = diabetes.target
df.head()

#2
df.info()
df.describe()

#3
from sklearn.model_selection import train_test_split
X = df.drop(columns=['target'])
y = df['target']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

print(f"Training set size: {X_train.shape}")
print(f"Testing set size: {X_test.shape}")

"""Task-2 Linear Regression Model"""

from sklearn.linear_model import LinearRegression

model = LinearRegression()
model.fit(X_train, y_train)

print("Model Coefficients:", model.coef_)
print("Model Intercept:", model.intercept_)

#2
from sklearn.metrics import r2_score

y_pred = model.predict(X_test)
r2 = r2_score(y_test, y_pred)
print(f"R² Score on Test Set: {r2:.4f}")

"""Task 3: Implement Recursive Feature Elimination (RFE)"""

#1
from sklearn.feature_selection import RFE

rfe = RFE(estimator=model, n_features_to_select=1, step=1)
rfe.fit(X_train, y_train)

#2
import numpy as np

feature_ranking = pd.DataFrame({
    "Feature": X_train.columns,
    "Ranking": rfe.ranking_
}).sort_values(by="Ranking")

print(feature_ranking)

#3
import pandas as pd
from sklearn.feature_selection import RFE
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score

r2_scores = []
feature_coefficients = []
num_features = list(range(1, X_train.shape[1] + 1))


for n in num_features:
    rfe_n = RFE(estimator=model, n_features_to_select=n)
    rfe_n.fit(X_train, y_train)
    y_pred = rfe_n.predict(X_test)
    r2_scores.append(r2_score(y_test, y_pred))
    feature_coefficients.append(rfe_n.estimator_.coef_)

coef_df = pd.DataFrame(feature_coefficients, columns=X_train.columns, index=num_features)
r2_df = pd.DataFrame({"Number of Features": num_features, "R² Score": r2_scores})

print("\n R² Score Tracking")
print(r2_df)

print("\n Feature Coefficients")
print(coef_df)

#4
import matplotlib.pyplot as plt

plt.figure(figsize=(8, 5))
plt.plot(num_features, r2_scores, marker='o', linestyle='-', color='red')
plt.xlabel("Number of Retained Features")
plt.ylabel("R² Score")
plt.title("R² Score vs Number of Retained Features")
plt.grid(True)
plt.show()

#5
import numpy as np

r2_differences = np.diff(r2_scores)

threshold = 0.01
optimal_features_index = np.argmax(r2_differences < threshold) + 1

optimal_features = num_features[optimal_features_index]

print(f"Optimal number of features: {optimal_features}")

"""Task 4: Analyze Feature Importance

"""

#1
import pandas as pd

coef_df = pd.DataFrame(feature_coefficients, columns=X_train.columns, index=num_features)
coef_df = coef_df.fillna(0)
print("\nFeature Coefficients at Each Iteration of RFE")
print(coef_df)

#2
import pandas as pd

optimal_feature_coefficients = coef_df.loc[optimal_features].abs()
most_important_features = optimal_feature_coefficients.nlargest(3).index.tolist()

print(f"Top 3 Most Important Features: {most_important_features}")

important_features_df = coef_df[most_important_features]

print("\nFeature Coefficients of Top 3 Features at Each Iteration")
print(important_features_df)

#3
import pandas as pd
import numpy as np

initial_ranking = rfe_n.ranking_

feature_ranking_df = pd.DataFrame({"Feature": X_train.columns, "Initial Rank": initial_ranking})

feature_ranking_df = feature_ranking_df.sort_values("Initial Rank")

final_selected_features = most_important_features
feature_ranking_df["Final Selected"] = feature_ranking_df["Feature"].isin(final_selected_features)

print("\nInitial Feature Ranking vs Final Selected Features")
print(feature_ranking_df)

"""Task 5: Reflection"""

"""1.
It progressively eliminates the least significant features, enhancing model interpretability while preserving predictive performance. The R² score on the test set (0.4526) suggests a decent fit but highlights the potential benefits of exploring non-linear models for improved accuracy. 

"""
"""
2.RFE iteratively eliminates features based on their contribution to model performance, while LASSO applies L1 regularization, shrinking some coefficients to zero. RFE ranks features explicitly, whereas LASSO removes them dynamically based on the regularization strength. Both methods help mitigate overfitting, but LASSO is more effective in high-dimensional datasets due to its automatic feature selection.
"""

"""
3.
The most influential features, s1, s5, and bmi, play a crucial role in predicting diabetes progression. s1 and s5 exhibit the highest absolute coefficients, indicating a strong correlation with disease severity. bmi, a well-known health indicator, reinforces its relevance in diabetes prediction by capturing the impact of body mass on metabolic health. Additionally, some initially high-ranked features, such as age, were removed during the selection process, suggesting they contribute less predictive value within this dataset.
"""
