{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "cf8a3c62-216c-446d-851d-7249cc67787a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "cf8a3c62-216c-446d-851d-7249cc67787a",
        "outputId": "76153ddb-38f0-4a60-9d54-818242123075"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "table {align:left;display:block} \n",
              "</style>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "%%html\n",
        "<style>\n",
        "table {align:left;display:block}\n",
        "</style>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "91e27c8c-7f53-4e9f-b4da-7e0343fb6313",
      "metadata": {
        "id": "91e27c8c-7f53-4e9f-b4da-7e0343fb6313"
      },
      "source": [
        "# Markov Decision Process (MDP)\n",
        "----\n",
        "\n",
        "**Value Iteration Process with Policy Changes in MDP**\n",
        "\n",
        "We begin with a Markov Decision Process (MDP) where an agent decides whether to invest conservatively (C) or aggressively (A) in a financial portfolio. The objective is to find an optimal policy maximizing long-term rewards.\n",
        "\n",
        "---\n",
        "\n",
        "### **Step 1: Defining the MDP Components**\n",
        "\n",
        "**States (S):**\n",
        "\n",
        "- Low Wealth (L)\n",
        "- Medium Wealth (M)\n",
        "- High Wealth (H)\n",
        "\n",
        "**Actions (A):**\n",
        "\n",
        "- Conservative (C)\n",
        "- Aggressive (A)\n",
        "\n",
        "**Transition Probabilities:**\n",
        "\n",
        ">| Current State | Action | Next State Probabilities     |\n",
        "| ------------- | ------ | ---------------------------- |\n",
        "| Low (L)       | C      | 80% Stay in L, 20% Move to M |\n",
        "| Low (L)       | A      | 60% Stay in L, 40% Move to M |\n",
        "| Medium (M)    | C      | 70% Stay in M, 30% Move to H |\n",
        "| Medium (M)    | A      | 50% Stay in M, 50% Move to H |\n",
        "| High (H)      | C      | 90% Stay in H, 10% Drop to M |\n",
        "| High (H)      | A      | 70% Stay in H, 30% Drop to M |\n",
        "\n",
        "**Rewards:**\n",
        "\n",
        "- Low Wealth (L): -1\n",
        "- Medium Wealth (M): 3\n",
        "- High Wealth (H): 5\n",
        "\n",
        "**Discount Factor (γ):** 0.9\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4bf6becd-538b-4010-b4ed-c2e03c6c2e1a",
      "metadata": {
        "id": "4bf6becd-538b-4010-b4ed-c2e03c6c2e1a"
      },
      "source": [
        "### **Step 2: Value Iteration Updates**\n",
        "\n",
        "We initialize values: $V_0(L) = 0$, $V_0(M) = 0$, $V_0(H) = 0$.\n",
        "\n",
        "#### **Iteration 1**\n",
        "\n",
        "Using Bellman’s equation:\n",
        "\n",
        ">$\n",
        "V_1(s) = \\max_a \\left[ R(s) + \\gamma \\sum_{s'} P(s' | s, a) V_0(s') \\right]\n",
        "$\n",
        "\n",
        "For **Low Wealth (L):**\n",
        "\n",
        ">$\n",
        "V_1(L) = \\max \\left[ -1 + 0.9(0.8V_0(L) + 0.2V_0(M)), -1 + 0.9(0.6V_0(L) + 0.4V_0(M)) \\right]\n",
        "$\n",
        "\n",
        "For **Medium Wealth (M):**\n",
        "\n",
        ">$\n",
        "V_1(M) = \\max \\left[ 3 + 0.9(0.7V_0(M) + 0.3V_0(H)), 3 + 0.9(0.5V_0(M) + 0.5V_0(H)) \\right]\n",
        "$\n",
        "\n",
        "For **High Wealth (H):**\n",
        "\n",
        ">$\n",
        "V_1(H) = \\max \\left[ 5 + 0.9(0.9V_0(H) + 0.1V_0(M)), 5 + 0.9(0.7V_0(H) + 0.3V_0(M)) \\right]\n",
        "$\n",
        "\n",
        "Since $V_0(L) = V_0(M) = V_0(H) = 0$, the initial values are just the rewards.\n",
        "\n",
        ">$\n",
        "V_1(L) = -1, \\quad V_1(M) = 3, \\quad V_1(H) = 5\n",
        "$\n",
        "\n",
        "#### **Policy Evaluation after Iteration 1**\n",
        "\n",
        "> \\$\n",
        "Q(L, C) = -1 + 0.9(0.8(-1) + 0.2(3)) = -1.18\n",
        "\\$\n",
        "\n",
        "> \\$\n",
        "Q(L, A) = -1 + 0.9(0.6(-1) + 0.4(3)) = -0.46\n",
        "\\$\n",
        "\n",
        "> \\$\n",
        "(M, C) = 3 + 0.9(0.7(3) + 0.3(5)) = 6.24\n",
        "\\$\n",
        "\n",
        "> \\$\n",
        "Q(M, A) = 3 + 0.9(0.5(3) + 0.5(5)) = 6.6\n",
        "\\$\n",
        "\n",
        "> \\$\n",
        "Q(H, C) = 5 + 0.9(0.9(5) + 0.1(3)) = 9.32\n",
        "\\$\n",
        "\n",
        "> \\$\n",
        "Q(H, A) = 5 + 0.9(0.7(5) + 0.3(3)) = 8.96\n",
        "\\$\n",
        "\n",
        "**Policy at Iteration 1:**\n",
        "- L → Conservative (C)\n",
        "- M → Aggressive (A)\n",
        "- H → Conservative (C)\n",
        "\n",
        "\n",
        "#### **Iteration 2**\n",
        "\n",
        "Updating $V_2(s)$:\n",
        "\n",
        ">$\n",
        "V_2(L) = \\max \\left[ -1 + 0.9(0.8(-1) + 0.2(3)), -1 + 0.9(0.6(-1) + 0.4(3)) \\right]\n",
        "$\n",
        "\n",
        ">$\n",
        "V_2(H) = \\max \\left[ 5 + 0.9(0.9(5) + 0.1(3)), 5 + 0.9(0.7(5) + 0.3(3)) \\right]\n",
        "$\n",
        "\n",
        "Computing these:\n",
        "\n",
        ">$\n",
        "V_2(L) = -0.46, \\quad V_2(M) = 6.6, \\quad V_2(H) = 9.32\n",
        "$\n",
        "\n",
        "#### **Policy Evaluation after Iteration 2**\n",
        "\n",
        "\n",
        "##### From state is: L\n",
        "\n",
        "> \\$\n",
        "Q(L, C) = -1 + 0.9(0.8(-0.46) + 0.2(6.6)) = -0.1432\n",
        ">\\$\n",
        "\n",
        "> \\$\n",
        "Q(L, A) = -1 + 0.9(0.6(-0.46) + 0.4(6.6)) = 1.1276\n",
        "\\$\n",
        "\n",
        "##### From state is: M\n",
        "\n",
        "> \\$\n",
        "Q(M, C) = 3 + 0.9(0.7(6.6) + 0.3(9.32)) = 9.6744\n",
        "\\$\n",
        "\n",
        "> \\$\n",
        "Q(M, A) = 3 + 0.9(0.5(6.6) + 0.5(9.32)) = 10.164\n",
        "\\$\n",
        "\n",
        "##### From state is: H\n",
        "\n",
        "> \\$\n",
        "Q(H, C) = 5 + 0.9(0.9(9.32) + 0.1(6.6)) = 13.1432\n",
        "\\$\n",
        "\n",
        "> \\$\n",
        "Q(H, A) = 5 + 0.9(0.7(9.32) + 0.3(6.6)) = 12.6536\n",
        "\\$\n",
        "\n",
        "**Policy at Iteration 2:**\n",
        "- L → Aggressive (A)\n",
        "- M → Aggressive (A)\n",
        "- H → Conservative (C)\n",
        "\n",
        "#### **Iteration 3**\n",
        "\n",
        "Updating $V_3(s)$:\n",
        "\n",
        ">$\n",
        "V_3(L) = \\max \\left[ -1 + 0.9(0.8(-0.46) + 0.2(6.6)), -1 + 0.9(0.6(-0.46) + 0.4(6.6)) \\right]\n",
        "$\n",
        "\n",
        ">$\n",
        "V_3(H) = \\max \\left[ 3 + 0.9(0.7(6.6) + 0.3(9.32)), 3 + 0.9(0.5(6.6) + 0.5(9.32)) \\right]\n",
        "$\n",
        "\n",
        ">$\n",
        "V_3(M) = \\max \\left[ 5 + 0.9(0.9(9.32) + 0.1(6.6)), 5 + 0.9(0.7(9.32) + 0.3(6.6)) \\right]\n",
        "$\n",
        "\n",
        "Computing these:\n",
        "\n",
        ">$\n",
        "V_3(L) = 1.1276, \\quad V_3(M) = 10.164, \\quad V_3(H) = 13.1432\n",
        "$\n",
        "\n",
        "#### **Policy Change Analysis**\n",
        "\n",
        "From **Iteration 2 to Iteration 3**, let’s check the action values to determine if the policy changed.\n",
        "\n",
        "For **Low Wealth (L):**\n",
        "\n",
        ">$\n",
        "$Q(L, C) = -1 + 0.9(0.8(1.1276) + 0.2(10.164)) = 1.641392$\n",
        "$\n",
        "\n",
        ">$\n",
        "Q(L, A) = -1 + 0.9(0.6(1.1276) + 0.4(10.164)) = 3.267944\n",
        "$\n",
        "\n",
        "For **Medium Wealth (M):**\n",
        "\n",
        ">$\n",
        "Q(M, C) = 3 + 0.9(0.7(10.164) + 0.3(13.1432)) = 12.951984\n",
        "$\n",
        "\n",
        ">$\n",
        "Q(M, A) = 3 + 0.9(0.5(10.164) + 0.5(13.1432)) = 13.48824\n",
        "$\n",
        "\n",
        "For **High Wealth (H):**\n",
        "\n",
        ">$\n",
        "Q(H, C) = 5 + 0.9(0.9(13.1432) + 0.1(10.164)) = 16.560752\n",
        "$\n",
        "\n",
        ">$\n",
        "Q(H, A) = 5 + 0.9(0.7(13.1432) + 0.3(10.164)) = 16.024496\n",
        "$\n",
        "\n",
        "Since $Q(L, A) > Q(L, C)$ and $Q(H, C) > Q(H, A)$, the policy updates to:\n",
        "\n",
        "- **Low Wealth (L)** → Aggressive (A)\n",
        "- **Medium Wealth (M)** → Aggressive (A)\n",
        "- **High Wealth (H)** → Conservative (C)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1d0e5695-a01b-4bcc-909b-582b5e64d068",
      "metadata": {
        "id": "1d0e5695-a01b-4bcc-909b-582b5e64d068"
      },
      "source": [
        "### Summary: Policy Evolution Over Iterations\n",
        "\n",
        ">| State  | Iteration 1 | Iteration 2 | Iteration 3 |\n",
        "|--------|-------------|------------|------------|\n",
        "| Low    | -1          | -0.46      | 3.267944   |\n",
        "| Medium | 3           | 6.6        | 13.48824   |\n",
        "| High   | 5           | 9.32       | 16.560752  |\n",
        "\n",
        "This analysis shows how the agent is wealth changes over iterations. If the agent in the \"low\" state consistently makes the best decisions, their score improves from -1 to 3.26 by the third step, allowing them to keep increasing their wealth. Similarly, agents in the \"medium\" and \"high\" states experience continuous score growth when making optimal choices\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}